\section{Direct preference optimization (DPO)}

In the previous question we saw how we could train a model based on preference data. However, suppose you are given a pre-trained model and the corresponding preference data. Ideally, you would like to optimize the model directly on the preference data, instead of having to learn a reward function, and then run PPO on it. That is the idea behind direct preference optimization (DPO) \cite{NEURIPS2023_a85b405e}. The algorithm proposed in the original paper allows us to skip the reward learning and reinforcement learning steps, and optimize the model directly from preference data by optimizing the following loss:

$$\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right],$$

where $\pi_{\mathrm{ref}}$ is the policy from which we sampled $y_w$ and $y_l$ given $x$, $\pi_\theta$ is the policy we are optimizing, and $\sigma$ is the sigmoid function.

We will use DPO in this question as well.
To provide some context, let us consider the general approach for RLHF for text generation:

\begin{enumerate}
    \item Train a large language model (LLM) to do next token prediction given a context (the tokens that came previously).
    \item Given a fixed context $x$, generate possible next token sequence predictions $y_1$ and $y_2$, and store the triple $(x, y_1, y_2)$.
    \item Ask human supervisors to rank $y_1$ and $y_2$ given $x$ according to individual preference.
    \item Update the LLM to maximize the probability of giving the preferred answers using reinforcement learning.
\end{enumerate}

In a similar way, given an observation $x$ we could have two ranked sequences of actions $a^1_{1:T}$ and $a^2_{1:T}$, train the model to generate the preferred sequence of actions, and then execute them all\footnote{To understand why we are considering sequences of actions rather than a single action for the next time, recall that $25$ actions corresponded to $0.2$ seconds of video. If you found it difficult to rank a sequence of $25$ actions based on such a short video, imagine ranking the effect of a single action!}. If the length of the generated action sequence is equal to the environment time horizon, this is called open-loop control. However, this approach lacks robustness, since the plan of actions will not change in response to disturbances or compounding errors. Instead, we are going to adapt this into a more robust scheme by training our policy to predict a sequence of actions for the next $T$ time steps (where $T$ is the length of the trajectories in our preference dataset), but only take the first action in the plan generated by the policy. In this way, we re-plan our actions at every time step, ensuring the ability to respond to disturbances.

\begin{enumerate}[(a)]

	\input{03-dpo/01-action-sequence}

	\input{03-dpo/02-update-sft}

	\input{03-dpo/03-update-dpo}

	\input{03-dpo/04-run-dpo}

	\input{03-dpo/05-render-policies}

\end{enumerate}